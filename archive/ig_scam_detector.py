# -*- coding: utf-8 -*-
"""IG Scam Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hvJulI6RbF1UlN6bow9-YxX7Louh3eyk

# Installing and Importing Dependencies
"""

#!pip install transformers

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

from sklearn.model_selection import train_test_split as tts
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup

import warnings
warnings.filterwarnings('ignore')

import math

"""# Setting Variables"""

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
EPOCHS=1 #4 #10
MAX_LEN=512
BATCH_SIZE=16

"""# Reading and Preprocessing Data"""

df = pd.read_csv('https://gist.githubusercontent.com/stefanerben/998f0be84ae3e483e79cfdfc2c32439d/raw/5c5c1489a13d73df13af0bdc3ab4f2ba82c466e8/comments-rating.csv')[['msg', 'scam']]
df.head()

df.shape

class ScamCollectionDataset(Dataset):
    def __init__(self, scam, msgs, tokenizer, max_len):
        self.msgs = msgs
        self.scam = scam
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.msgs)

    def __getitem__(self, i):
        msg = str(self.msgs[i])
        scam = self.scam[i]

        encoding = self.tokenizer.encode_plus(
            msg, 
            add_special_tokens=True,
            max_length=self.max_len,
            truncation=True,
            return_token_type_ids=False,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'msg': msg,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'scam': torch.tensor(scam, dtype=torch.long)
        }

def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = ScamCollectionDataset(
        scam=df['scam'].to_numpy(),
        msgs=df['msg'].to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
    )

    return DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=4
    )

df_train, df_test = tts(
    df,
    test_size=0.2,
    random_state=42
)
df_val, df_test = tts(
    df_test,
    test_size=0.5,
    random_state=42
)
df_train.shape, df_test.shape, df_val.shape

print(df_train)

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)

d = next(iter(train_data_loader))
d.keys()

d['input_ids'].shape, d['attention_mask'].shape, d['scam'].shape

"""# Model Building"""
print("Model Building")
class ScamClassifier(nn.Module):
    def __init__(self, n_classes):
        super(ScamClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )[1]
        output = self.drop(pooled_output)
        return self.out(output)

model = ScamClassifier(n_classes=2)
model = model.to(device)

"""# Loss, Optimizer and Scheduler"""
print("Loss, Optimizer and Scheduler")
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)

total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss().to(device)

# input_ids = d['input_ids'].to(device)
# attention_mask = d['attention_mask'].to(device)
# targets = d['scam'].to(device)

# outputs = model(
#     input_ids=input_ids,
#     attention_mask=attention_mask
# )
# loss = loss_fn(outputs, targets)
# loss

"""# Train Model Function"""

def train(
    model,
    loss_fn,
    optimizer,
    scheduler,
    device,
    data_loader,
    n_examples
):
    model = model.train() # Setting Model in training mode

    losses = []
    correct_predictions = 0

    for d in data_loader:
        input_ids = d['input_ids'].to(device) # [16, 512]
        attention_mask = d['attention_mask'].to(device) # [16, 512]
        targets = d['scam'].to(device) # [16]

        # Forward Propogation
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        ) # [16, 3]

        # Calculating Loss
        loss = loss_fn(outputs, targets)

        _, preds = torch.max(outputs, dim=1)

        correct_predictions += torch.sum(preds == targets)
        losses.append(loss.item())
        
        # Backward Propogation
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clipping Gradient (Exploding Gradient Problem)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad() # Resetting gradients

    train_acc = correct_predictions.double() / n_examples
    train_loss = np.mean(losses)
    
    return train_acc, train_loss

"""# Validating Model Function"""
print("Validating Model Function")
def evaluate_model(
    model,
    loss_fn,
    device,
    data_loader,
    n_examples   
):
    model = model.eval() # Setting Model in evaluation mode

    losses = []
    correct_predictions = 0

    with torch.no_grad():
        for d in data_loader:
            input_ids = d['input_ids'].to(device) # [16, 512]
            attention_mask = d['attention_mask'].to(device) # [16, 512]
            targets = d['scam'].to(device) # [16]

            # Forward Propogation
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            ) # [16, 3]

            # Calculating Loss
            loss = loss_fn(outputs, targets)

            _, preds = torch.max(outputs, dim=1)

            correct_predictions += torch.sum(preds == targets)
            losses.append(loss.item())
        
    train_acc = correct_predictions.double() / n_examples
    train_loss = np.mean(losses)

    return train_acc, train_loss

"""# Training the Model"""
print("Training the Model")
history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train(
        model,
        loss_fn,
        optimizer,
        scheduler,
        device,
        train_data_loader,
        len(df_train)
    )

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = evaluate_model(
        model,
        loss_fn,
        device,
        val_data_loader,
        len(df_val)
    )

    print(f'Validation loss {val_loss} accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc

# Evaluating the Model Performance
print("Evaluating the Model Performance")
test_acc, _ = evaluate_model(
    model,
    loss_fn,
    device,
    test_data_loader,
    len(df_test)
)
test_acc.item()

new_history = defaultdict(list)

new_history['train_acc'] = [ x.cpu() for x in history['train_acc'] ]
new_history['val_acc'] = [ x.cpu() for x in history['val_acc'] ]

plt.figure(figsize=(10, 7))
plt.plot(new_history['train_acc'], label='train accuracy')
plt.plot(new_history['val_acc'], label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

def get_predictions(model, data_loader):
    model = model.eval()

    msgs = []
    predictions = []
    predictions_probs = []
    real_values = []

    with torch.no_grad():
        for d in data_loader:
            msg = d['msg']
            input_ids = d['input_ids'].to(device)
            attention_masks = d['attention_mask'].to(device)
            scam = d['scam'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_masks
            )

            _, preds = torch.max(outputs, dim=1)

            probs = torch.nn.functional.softmax(outputs, dim=1)

            msgs.extend(msg)
            predictions.extend(preds)
            predictions_probs.extend(probs)
            real_values.extend(scam)
    predictions = torch.stack(predictions).cpu()
    predictions_probs = torch.stack(predictions_probs).cpu()
    real_values = torch.stack(real_values).cpu()
    return msgs, predictions, predictions_probs, real_values

y_msgs, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)


# Creating Final Classification Report
def create_classification_report(Y_test, Y_pred):
    print('--------Classification Report---------\n')
    accuracy = accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    roc_auc = roc_auc_score(Y_test, Y_pred)
    metrices = [accuracy, f1, precision, recall, roc_auc]
    scores = pd.DataFrame(pd.Series(metrices).values, index=['accuracy', 'f1-score', 'precision', 'recall', 'roc auc score'], columns=['score'])
    print(scores)
    print('\n--------Plotting Confusion Matrix---------')
    sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True, cmap='RdYlGn_r', annot_kws={'size': 16})
    return scores

create_classification_report(y_test, y_pred)

# Get prediction for a single message
def get_single_prediction(
    model, tokenizer, message
):
    model = model.eval()

    message = tokenizer.encode_plus(
        message,
        max_length=MAX_LEN,
        add_special_tokens=True,
        return_token_type_ids=False,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids = message['input_ids'].to(device)
    attention_mask = message['attention_mask'].to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
    )

    _, prediction = torch.max(outputs, dim=1)

    return prediction.item()

#message = "This is a test message"
message = "Invest with our BTC manager"
result = get_single_prediction(model, tokenizer, message)
print(result)

# predict if a message is scam or not
def predict_scam(model, msg):
    model = model.eval()
    msg = [msg]
    encoding = tokenizer.encode_plus(
        msg,
        max_length=MAX_LEN,
        add_special_tokens=True,
        return_token_type_ids=False,
        padding=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    output = model(
        input_ids=input_ids,
        attention_mask=attention_mask
    )

    _, prediction = torch.max(output, dim=1)

    return prediction.item()

# Predicting if a message is scam or not
msg = "Congratulations! You have won a free ticket to the Bahamas! Call 1-800-555-1234 to claim your prize!"
msg = "Santos coin follow"
result = predict_scam(model, msg)
print(result)

data_example = [
    ['Hi this is a test', 0],
    ['Santos coin follow', 1],
    ['Casino coin follow', 1],
    ['Forgive me for doing this but If I"m not talking the company legitimacy then I"m ungrateful, I only invested $1000 and cut a profit of $10,000 I was like learning to the air because it has been a long time I heard people meeting with a trustful manager like Alexander I will never stop talking about your good work God blesss you always Ma, dm her now @_katherina_fx_expert_trader_', 0],
    ['Waking up every day to see my balance increased is the best ever happended to me. Thank you so much @harry87', 1]
    ]
df_example = pd.DataFrame(data_example, columns=['msg', 'scam'])
example_data_loader = create_data_loader(df_example, tokenizer, MAX_LEN, BATCH_SIZE)

y_ex_msgs, y_ex_pred, y_ex_pred_probs, y_ex_test = get_predictions(
  model,
  example_data_loader
)

for x in range(len(y_ex_msgs)): 
  print(round(y_ex_pred_probs[x][1].item(), 4), y_ex_msgs[x])

#print(y_ex_msgs)
#print(y_ex_pred[2].item())
#print(y_ex_pred_probs[2][1])
#print(y_ex_test)

"""# Comment Rater"""

exit()
#from google.colab import drive
#drive.mount('/content/drive')

import random
import json
import requests
import torch
from transformers import BertTokenizer, BertForSequenceClassification


# Set the device to CPU
device = 'cpu'

# Set the maximum length of the messages
max_len = 50

#@title
# define pretrained model name
PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)

class ScamClassifier(nn.Module):
    def __init__(self, n_classes):
        super(ScamClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )[1]
        output = self.drop(pooled_output)
        return self.out(output)


# Load the saved model
model = ScamClassifier(n_classes=2)
#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False)

model.load_state_dict(torch.load('best_model_state.bin', map_location=torch.device('cpu')), strict=False)
model.eval()

# Define a function for evaluating a single comment text
def evaluate_comment(comment):
    encoded_comment = tokenizer.encode_plus(comment, add_special_tokens=True, max_length=max_len, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')
    input_ids = encoded_comment['input_ids'].to(device)
    attention_mask = encoded_comment['attention_mask'].to(device)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    probs = torch.nn.functional.softmax(outputs[0], dim=1).cpu().numpy()
    pred = torch.argmax(outputs[0], dim=1).cpu().numpy()[0]
    prediction = 'Spam' if pred == 1 else 'Not Spam'
    probability = probs[0][1]

    return prediction, probability

comments_example = [
    ['Hi this is a test'],
    ['Santos coin follow'],
    ['Casino coin follow'],
    ['Follow Mani Coin now'],
    ['Waking up every day to see my balance increased is the best ever happended to me. Thank you so much @harry87']
    ]
    
    
for comment_text in comments_example:   
    prediction, probability = evaluate_comment(comment_text)
    score = int(round(probability*100))
    print(score, comment_text)

